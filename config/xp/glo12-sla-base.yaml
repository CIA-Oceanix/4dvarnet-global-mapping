# @package _global_

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${xpname}-${now:%H-%M-%S}
  callbacks:
    LogJobReturnCallback:
      _target_: hydra.experimental.callbacks.LogJobReturnCallback

domain: ???
ckpt: null
xpname: glo12-sla

trainer:
  _target_: pytorch_lightning.Trainer
  inference_mode: false
  deterministic: true
  gradient_clip_val: 0.5
  accelerator: gpu
  devices: 1
  logger:
    _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    save_dir: ${hydra:runtime.output_dir}
    name: ${hydra:runtime.choices.xp}
    version: ''
  max_epochs: 1000
  limit_train_batches: 100  #Â 640
  # accumulate_grad_batches: 8
  callbacks:
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_mse
      save_last: true
      save_top_k: 3
      filename: '{val_mse:.5f}-{epoch:03d}'

datamodule:
  _target_: contrib.glorys12.DistinctNormDataModule
  input_da:
    _target_: contrib.glorys12.load_glorys12_data_on_fly_inp #contrib.glorys12.load_glorys12_data
    tgt_path: /SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th.nc #/Odyssey/public/glorys/reanalysis/glorys12_2010_2019_daily_sla_4th.nc #glorys4_2010_2019_sla.nc
    tgt_var: sla
    inp_path: /SCRATCH/rfablet/Dataset/mask_6nadirs_4th.nc
    inp_var: inp
    #inp_path: /SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_input.nc #/Odyssey/public/glorys/reanalysis/glorys12_2010_2019_daily_sla_4th_input.nc #glorys4_2010_2019_input_sla.nc
    #inp_var: sla
  domains:
    train:
      time: {_target_: builtins.slice, _args_: ['2010-01-01', '2010-12-31']}
    val:
      time: {_target_: builtins.slice, _args_: ['2019-01-01', '2019-12-31']}
    test: null
  xrds_kw:
    train:
      patch_dims: {time: 15, lat: 232, lon: 366}
      strides: {time: 1, lat: 224, lon: 358}
      domain_limits: ${domain.train}
    val:
      patch_dims: {time: 15, lat: 680, lon: 1440}
      strides: {time: 1, lat: 1, lon: 1}
      domain_limits: ${domain.train}
  dl_kw: {batch_size: 16, num_workers: 1}
  norm_stats:  # norm values from tgt
    train:
    - -0.001930378327255099
    - 0.08604745118775728
    val:
    - 0.01560162689028551
    - 0.08583080433051968

model:
  _target_: contrib.glorys12.Lit4dVarNetIgnoreNaN
  w_mse: 50.
  w_grad_mse: 1000.
  w_prior: 1.
  persist_rw: false
  opt_fn:
    _target_: ocean4dvarnet.utils.cosanneal_lr_adam
    _partial_: true
    lr: 1e-4
    T_max: ${trainer.max_epochs}
  rec_weight:
    _target_: ocean4dvarnet.utils.get_triang_time_wei
    patch_dims: ${datamodule.xrds_kw.train.patch_dims}
    crop: {time: 0, lat: 4, lon: 4}
  val_rec_weight:
    _target_: ocean4dvarnet.utils.get_triang_time_wei
    patch_dims: ${datamodule.xrds_kw.val.patch_dims}
    crop: {time: 0, lat: 4, lon: 4}
  norm_stats: ${datamodule.norm_stats}

entrypoints:
  - _target_: pytorch_lightning.seed_everything
    seed: 333 #111 #
  - _target_: contrib.glorys12.train
    trainer: ${trainer}
    lit_mod: ${model}
    dm: ${datamodule}
    ckpt: ${ckpt}

defaults:
  - /domain: glorys
  - _self_
