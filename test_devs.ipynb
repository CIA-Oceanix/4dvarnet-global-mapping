{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d8df5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Odyssey/private/rfablet/miniforge3/envs/fdv/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import functools as ft\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import kornia.filters as kfilts\n",
    "import xarray as xr\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ocean4dvarnet.data import BaseDataModule, TrainingItem\n",
    "from ocean4dvarnet.models import Lit4dVarNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f582d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_path ='/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th.nc' #/Odyssey/public/glorys/reanalysis/glorys12_2010_2019_daily_sla_4th.nc #glorys4_2010_2019_sla.nc\n",
    "inp_path = '/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_input.nc'\n",
    "tgt_var = 'sla'\n",
    "inp_var = 'sla'\n",
    "\n",
    "isel1 = dict(time=slice('2010-01-01', '2019-12-31'))\n",
    "isel2 = dict(time=slice('2018-01-01', '2019-12-31'))\n",
    "\n",
    "tgt1 = xr.open_dataset(tgt_path)[tgt_var].sel(isel1)\n",
    "tgt2 = xr.open_dataset(tgt_path)[tgt_var].sel(isel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbce4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "isel = dict(time=slice('2018-01-01', '2019-12-31'))\n",
    "\n",
    "sla_glorys = xr.open_dataset(tgt_path)[tgt_var].sel(isel)\n",
    "obs_glorys = xr.open_dataset(inp_path)[inp_var].sel(isel)\n",
    "\n",
    "sla_glorys.to_netcdf(\"/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_2018-2019.nc\")\n",
    "obs_glorys.to_netcdf(\"/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_input_2018-2019.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828bc13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': slice('2018-01-01', '2019-12-31', None),\n",
       " 'time2': slice('2018-01-01', '2019-12-31', None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isel = dict(time=slice('2018-01-01', '2019-12-31'), time2=slice('2018-01-01', '2019-12-31'))\n",
    "\n",
    "isel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sla_glorys = xr.open_dataset(\"/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_2018-2019.nc\")[tgt_var].sel(isel)\n",
    "obs_glorys = xr.open_dataset(\"/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_input_2018-2019.nc\")[tgt_var].sel(isel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385b2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glorys12_data(tgt_path, inp_path, tgt_var=\"zos\", inp_var=\"input\"):\n",
    "    isel = None  # dict(time=slice(-465, -265))\n",
    "\n",
    "    _start = time.time()\n",
    "\n",
    "    print('..... Start loading dataset',flush=True)\n",
    "    \n",
    "    tgt = (\n",
    "        xr.open_dataset(tgt_path)[tgt_var]\n",
    "        .isel(isel)\n",
    "    )\n",
    "    inp = xr.open_dataset(inp_path)[inp_var].isel(isel)\n",
    "\n",
    "    ds = (\n",
    "        xr.Dataset(\n",
    "            dict(input=inp, tgt=(tgt.dims, tgt.values)),\n",
    "            inp.coords,\n",
    "        )\n",
    "        .to_array()\n",
    "        .sortby(\"variable\")\n",
    "    )\n",
    "\n",
    "    print(f\">>> Durée de chargement : {time.time() - _start:.4f} s\",flush=True)\n",
    "    #print(ds,flush=True)\n",
    "\n",
    "    #print(ds[\"tgt\"].data,flush=True)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2983d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..... Start loading dataset\n",
      ">>> Durée de chargement : 5.7619 s\n"
     ]
    }
   ],
   "source": [
    "tgt_path ='/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th.nc' #/Odyssey/public/glorys/reanalysis/glorys12_2010_2019_daily_sla_4th.nc #glorys4_2010_2019_sla.nc\n",
    "inp_path = '/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_input.nc'\n",
    "\n",
    "tgt_path ='/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_2018-2019.nc' #/Odyssey/public/glorys/reanalysis/glorys12_2010_2019_daily_sla_4th.nc #glorys4_2010_2019_sla.nc\n",
    "inp_path = '/SCRATCH/rfablet/Dataset/glorys12_2010_2019_daily_sla_4th_input_2018-2019.nc'\n",
    "\n",
    "#os.chdir('./contrib/glorys12/')\n",
    "ds = load_glorys12_data(tgt_path, inp_path, tgt_var=\"sla\", inp_var=\"sla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c49d4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocean4dvarnet.data import BaseDataModule, TrainingItem\n",
    "\n",
    "class DistinctNormDataModule(BaseDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.input_mask = None\n",
    "\n",
    "        if isinstance(self.input_da, (tuple, list)):\n",
    "            self.input_da, self.input_mask = self.input_da[0], self.input_da[1]\n",
    "\n",
    "    def norm_stats(self):\n",
    "        if self._norm_stats is None:\n",
    "            raise NormParamsNotProvided()\n",
    "        return self._norm_stats\n",
    "\n",
    "    def post_fn(self, phase):\n",
    "        m, s = self.norm_stats()[phase]\n",
    "\n",
    "        def normalize(item):\n",
    "            return (item - m) / s\n",
    "\n",
    "        return ft.partial(\n",
    "            ft.reduce,\n",
    "            lambda i, f: f(i),\n",
    "            [\n",
    "                TrainingItem._make,\n",
    "                lambda item: item._replace(tgt=normalize(item.tgt)),\n",
    "                lambda item: item._replace(input=normalize(item.input)),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "    def setup(self, stage=\"test\"):\n",
    "        self.train_ds = LazyXrDataset(\n",
    "            self.input_da.sel(self.domains[\"train\"]),\n",
    "            **self.xrds_kw[\"train\"],\n",
    "            postpro_fn=self.post_fn(\"train\"),\n",
    "            mask=self.input_mask,\n",
    "        )\n",
    "\n",
    "        self.val_ds = LazyXrDataset(\n",
    "            self.input_da.sel(self.domains[\"val\"]),\n",
    "            **self.xrds_kw[\"val\"],\n",
    "            postpro_fn=self.post_fn(\"val\"),\n",
    "            mask=self.input_mask,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_ds,\n",
    "            shuffle=False,\n",
    "            batch_size=1,\n",
    "            num_workers=1,\n",
    "        )\n",
    "\n",
    "class LazyXrDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        patch_dims,\n",
    "        domain_limits=None,\n",
    "        strides=None,\n",
    "        postpro_fn=None,\n",
    "        noise_type=None,\n",
    "        noise=None,\n",
    "        noise_spatial_perturb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.return_coords = False\n",
    "        self.postpro_fn = postpro_fn\n",
    "        self.ds = ds.sel(**(domain_limits or {}))\n",
    "        self.patch_dims = patch_dims\n",
    "        self.strides = strides or {}\n",
    "        _dims = (\"variable\",) + tuple(k for k in self.ds.dims)\n",
    "        _shape = (2,) + tuple(self.ds[k].shape[0] for k in self.ds.dims)\n",
    "        ds_dims = dict(zip(_dims, _shape))\n",
    "        # ds_dims = dict(zip(self.ds.dims, self.ds.shape))\n",
    "        self.ds_size = {\n",
    "            dim: max(\n",
    "                (ds_dims[dim] - patch_dims[dim]) // strides.get(dim, 1) + 1,\n",
    "                0,\n",
    "            )\n",
    "            for dim in patch_dims\n",
    "        }\n",
    "        self._rng = np.random.default_rng()\n",
    "        self.noise = noise\n",
    "        self.noise_spatial_perturb = noise_spatial_perturb\n",
    "\n",
    "        if noise_type is not None:\n",
    "            self.noise_type = noise_type\n",
    "        else:\n",
    "            self.noise_type = 'uniform-constant'\n",
    "\n",
    "        self.mask = kwargs.get(\"mask\")\n",
    "\n",
    "    def __len__(self):\n",
    "        size = 1\n",
    "        for v in self.ds_size.values():\n",
    "            size *= v\n",
    "        return size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "\n",
    "    def get_coords(self):\n",
    "        self.return_coords = True\n",
    "        coords = []\n",
    "        try:\n",
    "            for i in range(len(self)):\n",
    "                coords.append(self[i])\n",
    "        finally:\n",
    "            self.return_coords = False\n",
    "            return coords\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sl = {}\n",
    "        _zip = zip(\n",
    "            self.ds_size.keys(), np.unravel_index(item, tuple(self.ds_size.values()))\n",
    "        )\n",
    "\n",
    "        for dim, idx in _zip:\n",
    "            sl[dim] = slice(\n",
    "                self.strides.get(dim, 1) * idx,\n",
    "                self.strides.get(dim, 1) * idx + self.patch_dims[dim],\n",
    "            )\n",
    "\n",
    "        if self.mask is not None:\n",
    "            start, stop = sl[\"time\"].start % 365, sl[\"time\"].stop % 365\n",
    "            if start > stop:\n",
    "                start -= stop\n",
    "                stop = None\n",
    "            sl_mask = sl.copy()\n",
    "            sl_mask[\"time\"] = slice(start, stop)\n",
    "\n",
    "            da = self.ds.isel(**sl)\n",
    "\n",
    "            item = (\n",
    "                da.to_dataset(name=\"tgt\")\n",
    "                .assign(input=da.where(self.mask.isel(**sl_mask).values))\n",
    "                .to_array()\n",
    "                .sortby(\"variable\")\n",
    "            )\n",
    "        else:\n",
    "            item = self.ds.isel(**sl)\n",
    "\n",
    "        if self.return_coords:\n",
    "            return item.coords.to_dataset()[list(self.patch_dims)]\n",
    "\n",
    "        item = item.data.astype(np.float32)\n",
    "\n",
    "        #if self.noise:\n",
    "        #    noise = np.tile(\n",
    "        #        self._rng.uniform(-self.noise, self.noise, item[0].shape), (2, 1, 1, 1)\n",
    "        #    ).astype(np.float32)\n",
    "        #    item = item + noise\n",
    "\n",
    "        if self.noise is not None:\n",
    "\n",
    "            if self.noise_type ==  'uniform-constant' :\n",
    "                #noise =  self._rng.uniform(-self.noise, self.noise, item[0].shape).astype(np.float32)\n",
    "                #item[0] = item[0] + noise\n",
    "                noise =  self._rng.uniform(-self.noise, self.noise, item.shape).astype(np.float32)\n",
    "\n",
    "                item = item + noise\n",
    "            elif self.noise_type ==  'gaussian+uniform' :\n",
    "                scale = self._rng.uniform(0. , self.noise, 1).astype(np.float32)\n",
    "                noise = self._rng.normal(0., 1. , item[0].shape).astype(np.float32)\n",
    "\n",
    "                item[0] = item[0] + scale * noise\n",
    "            elif self.noise_type ==  'spatial-perturb' :\n",
    "                # patch dimensions\n",
    "                N = item[0].shape[1]\n",
    "                M = item[0].shape[2]\n",
    "                T = item[0].shape[0]\n",
    "\n",
    "                # parameters of the Gaussian Process\n",
    "                L = 5.0  # Spatial correlation length\n",
    "                T_corr = 20.0  # Temporal correlation length\n",
    "                sigma  = self._rng.uniform(0. , self.noise_spatial_perturb, 1).astype(np.float32)\n",
    "\n",
    "                # generate space-time random perturbations\n",
    "                w  = np.matmul( np.hanning( N ).reshape(N,1) , np.hanning( M ).reshape(1,M) )\n",
    "                dx = w * generate_correlated_fields_np(N, M, L, T_corr, sigma,T)\n",
    "                dy = w * generate_correlated_fields_np(N, M, L, T_corr, sigma,T)\n",
    "\n",
    "                # compute warped fields from reference field and associated error map\n",
    "                warped_field = np.zeros_like(field)\n",
    "\n",
    "                # Perform warping\n",
    "                for ii in range(T):\n",
    "                    warped_field[ii,:,:] = warp_field_np(item[1][ii,:,:], dx[ii,:,:], dy[ii,:,:])\n",
    "                \n",
    "                # residual error\n",
    "                error = item[1][ii,:,:] - warped_field\n",
    "\n",
    "                # apply mask\n",
    "                noise = np.where( np.isnan(item[0]) , np.nan, error )\n",
    "                print(\"..... std of the simulated noise : %.3f\"%np.nanstd(noise) )\n",
    "                \n",
    "                # adding a white noise\n",
    "                scale  = self._rng.uniform(0. , self.noise, 1).astype(np.float32)\n",
    "                wnoise = scale * self._rng.normal(0., 1. , item[0].shape).astype(np.float32)\n",
    "\n",
    "                item[0] = item[0] + noise + wnoise\n",
    "\n",
    "        if self.postpro_fn is not None:\n",
    "            return self.postpro_fn(item)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebfc924b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [-0.001930378327255099, 0.08604745118775728],\n",
       " 'val': [0.01560162689028551, 0.08583080433051968]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain = dict(train= dict(time=slice('2018-01-01', '2018-12-31')),\n",
    "               val= dict(time=slice('2019-01-01', '2019-12-31')))\n",
    "\n",
    "xrds_kw = dict(train=dict(patch_dims=dict(time=15,lat=232,lon=366),\n",
    "                          strides=dict(time=1,lat=224,lon=358),\n",
    "                          domain_limits=domain['train']),\n",
    "                val=dict(patch_dims=dict(time=15,lat=680,lon=1440),\n",
    "                          strides=dict(time=1,lat=1,lon=1),\n",
    "                          domain_limits=domain['train']),)\n",
    "               \n",
    "\n",
    "dl_kw = dict(batch_size=16, num_workers=1)\n",
    "norm_stats = dict(train=[-0.001930378327255099,0.08604745118775728],\n",
    "                  val=[0.01560162689028551, 0.08583080433051968])\n",
    "\n",
    "xrds_kw\n",
    "dl_kw\n",
    "norm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75e786f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DistinctNormDataModule(ds,domains=domain,xrds_kw=xrds_kw,dl_kw=dl_kw,norm_stats=norm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19946935",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8384326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataloader()\n",
    "len( dm.train_dataloader())\n",
    "#next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20211f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.setup(stage='val')\n",
    "dm.val_dataloader()\n",
    "\n",
    "len( dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5b3a159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DistinctNormDataModule at 0x75b4f5a675b0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
